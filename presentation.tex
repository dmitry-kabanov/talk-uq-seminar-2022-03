\documentclass[11pt,aspectratio=169,t]{beamer}
\usepackage{amsmath}
\usetheme[numbering=fraction]{metropolis}
\usepackage[T1]{fontenc}
\usepackage[default]{lato}
%\usepackage{mathpazo}
\usepackage{fourier}
% \usepackage{lmodern}
%\usepackage{concmath}
\usepackage{textcomp}
\usepackage{mathtools}
%\usepackage{skmath}
\usefonttheme{professionalfonts}
\usefonttheme[onlymath]{serif}

\overfullrule=2cm

\setbeamertemplate{footline}{%
  \begin{beamercolorbox}[wd=\textwidth, sep=1ex]{footline}%
    \usebeamerfont{page number in head/foot}%
    \usebeamertemplate*{frame footer}
    \hfill%
    \usebeamertemplate*{frame numbering}
    \quad% 
  \end{beamercolorbox}%
}

%\includeonlyframes{current}

\usepackage{algorithm2e}
\usepackage{siunitx}

% Table packages.
\usepackage{booktabs}

\usepackage{xparse}
\usepackage{makecell}
\usepackage{tikz}
\usepackage[beamer,customcolors]{hf-tikz}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{fadings}


\pdfstringdefDisableCommands{%
  \def\\{}%
  \def\texttt#1{<#1>}%
}

% ------------------------------------------------------------------------------
% Define useful colors.
\colorlet{black}{black!50!gray}
\colorlet{green}{green!50!gray}
\colorlet{blue}{blue!50!gray}

\colorlet{CBlue}{blue!15}
\colorlet{CBlueD}{blue!95!gray}
\colorlet{CRed}{red!15}
\colorlet{CRedD}{red!65!gray}
\colorlet{CGreenD}{green!60!gray}
\colorlet{COrange}{orange}

% Background color.
\definecolor{CBackground}{RGB}{255,253,253}
\setbeamercolor{background canvas}{fg=black,bg=CBackground}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
% tikz settings
\tikzset{   
    every path/.style={thick},
}
% ------------------------------------------------------------------------------

% \usepackage[protrusion,expansion,kerning,spacing,final]{microtype}
% \microtypecontext{spacing=nonfrench}

% ------------------------------------------------------------------------------
% Useful mathematical macros
\newcommand{\Data}{\vec{D}}
\newcommand{\DataExt}{\widetilde{\vec{D}}}
\newcommand{\MSE}{\ensuremath{\text{MSE}}}
\newcommand{\T}{\ensuremath{\text{T}}}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\VTheta}{\ensuremath{\vec{\theta}}}
\newcommand{\VLambda}{\ensuremath{\vec{\lambda}}}
%\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\R}{\mathbb R}
\newcommand{\UNN}[1][\text{NN}]{u_{#1}}
\newcommand{\FNN}[1][\text{NN}]{f_{#1}}
\newcommand{\NonlinOp}{\mathcal N\!}
\newcommand{\Loss}{\mathcal L}
\newcommand{\Model}{\mathcal G}
\newcommand{\ModelT}{\mathrm G}  % Total model: concatenation over dataset.
\newcommand{\given}{|}
\newcommand{\ParamSpace}{\mathcal U}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\newcommand{\dd}{\,\textrm{d}}
\renewcommand{\Pr}{\mathbb P}
\renewcommand{\hat}{\widehat}
\newcommand{\upot}{\vec u_{\text{pot}}}
\newcommand{\usol}{\vec u_{\text{sol}}}
\newcommand{\Grad}{\nabla}
\newcommand{\Div}{\nabla \cdot}
\newcommand{\Curl}{\nabla \! \times \!}

\newcommand{\xx}{\vec{x}}

\newcommand{\explain}[2]{\underset{\mathclap{\overset{\uparrow}{#2}}}{#1}}
\newcommand{\explainup}[2]{\overset{\mathclap{\underset{\downarrow}{#2}}}{#1}}

% Useful mathematical macros (end)
% ------------------------------------------------------------------------------

\title{Physics-informed neural networks for discrete Helmholtz--Hodge decomposition}
\author{Dmitry I.\ Kabanov, Luis Espath, Ra\'ul Tempone}
\institute{UQ hybrid seminar}
\date{1 March 2022}
\def\titlepage{%
  \usebeamertemplate{title page}%<---
}
\titlegraphic{\vspace{6cm}\hspace{4.3cm}\includegraphics{rwth-logo}\phantom{text}\par}

% ------------------------------------------------------------------------------

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{Introduction}
In 18?? Helmholtz postulated that a vector field $\vec u: \R^{n} \to \R^{n}$
can be decomposed in two fields:
$$
\vec u = \upot + \usol = \Grad \phi + \Curl \vec\psi
$$
where
\begin{itemize}
  \item $\upot := \Grad \phi$ potential (curl-free) subfield
       % , $\phi: \R^{n}\to\R$
  \item $\usol := \Curl \vec\psi$ solenoidal (divergence-free) subfield
        %, $\vec\psi: \R^{n}\to \R^{n}$
\end{itemize}

\onslide<2->{%
Vector calculus identities clarify the names ``curl-free'' and ``divergence-free'':
$$
\Curl \Grad \phi = 0 \quad \text{ for any } \phi
$$
and
$$
\Div \Curl \vec \psi = 0 \quad \text{ for any } \vec \psi
$$
}
\onslide<3->{%
Nowadays it is called Helmholtz--Hodge decomposition (HHD).
}

\end{frame}
% ------------------------------------------------------------------------------
\begin{frame}{HHD, in general, is not unique}
If $\vec u$ is smooth, then by taking divergence, we find that
$$
\Delta \phi = \Div \vec u
$$
and
$$
\usol = \vec u - \Grad \phi.
$$

However, the decomposition is not unique. For any harmonic function $\xi$,
$$
\vec u = \Grad (\phi + \xi) + \left( \Curl \vec \psi - \Grad \xi \right)
$$
is also a decomposition.

If domain $D=\R^{n}$ and $\vec u$ vanishes as $|\vec x| \to \infty$, then
$\xi \equiv 0$, and the decomposition is unique.

\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{Orthogonal HHD}
Another way to achieve uniqueness is to impose additional constraints.

For example orthogonality of the subfields in $L^{2}$-sense
\[
\int_{D} \upot \usol \, \dd \vec x = 0
\]
Orthogonality can be achieved by imposing boundary conditions on the flow.
\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{Orthogonality of subfields}

\emph{Theorem. Orthogonality of subfields [ChorinMardsen2000]}.
A~vector field $\vec u$ on $D$ can be decomposed
in the form
\[
\vec u = \upot + \usol = \Grad \phi + \usol
\]
with $\Div \usol = 0$.
If $\usol \cdot \vec n = 0$ on $\partial D$, then
$
\int_{D} \upot \usol \, \dd \vec x = 0
$

\onslide<2->{%
\emph{Proof}.
\begin{align*}
  \Div \left(\phi \usol \right)
  &= \phi \Div \usol + \usol \cdot \Grad \phi && \text{divergence of product} \\
  &= \usol \cdot \Grad \phi && \Div \usol = 0
\end{align*}
}
\onslide<3->{%
\begin{align*}
  \int_{D} \usol \cdot \Grad \phi \, \dd \vec x
  &= \int_{D} \Div \left(\phi \usol\right) \, \dd \vec x && \text{by the above} \\
  &= \oint_{\partial D} \phi \usol \cdot \vec n \, \dd S && \text{Gauss--Ostrogradsky thm}\\
  &= 0 && \usol \cdot \vec n = 0 \text{ on } \partial D
\end{align*}
}

\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{Uniqueness of orthogonal HHD}
\emph{Theorem. Uniqueness of orthogonal HHD [ChorinMardsen2000]}.
A~vector field $\vec u$ on $D$ can be decomposed
in the form
\[
\vec u = \upot + \usol = \Grad \phi + \usol
\]
with $\Div \usol = 0$.
If $\usol \cdot \vec n = 0$ on $\partial D$, then the decomposition is unique.

Proof.
Let
$$
\vec u = \upot + \usol = \Grad \phi + \usol =
    \Grad \phi' + \usol' = \upot' + \usol'
$$
Both decompositions must be orthogonal
$$
\int_D \Grad \phi \cdot \usol \, \dd \xx = 0 \text{ and }
\int_D \Grad \phi' \cdot \usol' \, \dd \xx = 0.
$$
\end{frame}

%-------------------------------------------------------------------------------
\begin{frame}{Uniqueness of orthogonal HHD}
\emph{Proof (continuation).}
Having
$
\vec u = \Grad \phi + \usol =
    \Grad \phi' + \usol'
$
implies
$$
0 = \Grad \left(\phi - \phi'\right) + \usol - \usol'
$$
and by taking the inner product with $\usol - \usol'$ over $D$, we get
\begin{align*}
0 &= - \int_D \Grad \phi \cdot \usol' + \Grad \phi' \cdot \usol +
    \norm{\usol - \usol'}{}^2 \,\dd \xx \\
  &= - \int_D \Div \left(\phi \usol'\right) + \Div \left(\phi' \usol\right) +
    \norm{\usol - \usol'}{}^2 \,\dd \xx && \Div \usol = \Div \usol' = 0\\
  &= - \oint_{\partial D} \phi \usol' \cdot \vec n
     + \phi' \usol \cdot \vec n \,\dd s
     + \int_D \norm{\usol - \usol'}{}^2 \,\dd \xx && \text{Gauss--Ostrogradsky thm}\\
  &= \phantom{-}\int_D \norm{\usol - \usol'}{}^2 \,\dd \xx,
\end{align*}
which implies that $\usol = \usol'$, and hence, $\Grad \phi = \Grad \phi'$.
\end{frame}

% ------------------------------------------------------------------------------
\section{Discrete Helmholtz--Hodge decomposition}

% ------------------------------------------------------------------------------
\begin{frame}{Discrete Helmholtz--Hodge decomposition}

\begin{minipage}{0.98\textwidth}
Consider a problem in which vector field $\vec u$ is not given analytically
but only as a finite dataset $\mathrm D = (\xx_{i}, \vec u_{i})$ for $i=1,\dots, N$ inside
a~given compact domain $D$.

In this case, we want to learn the appropriate decomposition via empirical
risk minimization with additional regularizing terms
\[
\phi^{*}, \psi^{*} =
\arg \min_{\phi, \psi} \mathcal L(\phi, \vec \psi, \mathrm D)
  + \lambda R_{\perp}(\phi, \vec \psi) + \mu R_{H^{3}}(\phi, \vec \psi)
\]
where
\[
\mathcal L(\phi, \vec \psi, \mathrm{D}) =
\frac1N \sum_{i=1}^{N} \norm{\Grad \phi(\xx) + \Curl{\vec \psi(\xx)} - \vec u}^{2}_{2}
\]
\end{minipage}
\hfill
\begin{minipage}{0.02\textwidth}
  \centering
  \rule{\textwidth}{\textwidth}
\end{minipage}
\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}{Discrete Helmholtz--Hodge decomposition with regularizers}
Using the above results on the uniqueness of the orthogonal HHD, we solve
in~practice the following inverse problem with additional regularizers
\[
\phi^{*}, \psi^{*} =
\arg \min_{\phi, \psi} \mathcal L(\phi, \vec \psi, \mathrm D)
  + \lambda R_{\perp}(\phi, \vec \psi) + \mu R_{\text S}(\phi, \vec \psi)
\]
where
\begin{align*}
&\text{orthogonality regularizer }
&R_{\perp}(\phi, \vec \psi) &=
    \int_{D} \left(\Grad \phi \cdot \Curl \vec \psi \right)^{2} \, \dd \xx \\
&\text{smoothness regularizer }
&R_{\text S}(\phi, \vec \psi) &=
    \int_{D} \norm{\partial^{3}\phi(\xx)}^{2} + \norm{\partial^{3}\vec \psi(\xx)}^{2} \, \dd \xx
\end{align*}

Regularizers are discretized on uniform grid $32\times32$.

Hyperparameters $\lambda$ and $\mu$ should be set before the training.
We use $\mu=1e-2$ in all experiments but we vary $\lambda$ to study its effect
on the reconstruction performance.

\end{frame}

\begin{frame}{Specialization to 2D}
We consider only two-dimensional case $D \subset \R^{2}$.

Then the vector potential $\vec \psi$ is a vector that is orthogonal to $D$:
\[
 \vec \psi = (0, 0, \psi)
\]
and the curl operator becomes
\[
\Curl \vec \psi = \partial_{2} \psi \vec e_{1} - \partial_{1} \psi \vec e_{2}
\]
where $(\vec e_{1}, \vec e_{2})$ is the canonical basis in $\R^{2}$.

Function $\psi$ is known as \emph{stream function} in fluid mechanics.

Note that 2D decomposition is now defined by two scalar-valued functions $\phi$
and $\psi$.
\end{frame}


% ------------------------------------------------------------------------------
\begin{frame}
\frametitle{Feed-forward neural networks}

As estimating functions $\phi$ and $\psi$ we use two
fully-connected neural networks (multilayer perceptrons).

NN with \(L\) hidden layers is a mapping from
\(\R^{2}\) to \(\R\)
of the following compositional structure:
\[
    \phi(\xx; \VTheta) = A \circ F_{L} \circ \dots \circ F_1(\xx)
\]
where
\begin{itemize}
    \item \(A\) is linear operator, which is also called \textbf{output layer}
    \item \(F_j : \R^{n_{j-1}} \to \R^{n_j} \) nonlinear continuous
    functions, with \(n_0 = 2\), which are called \textbf{hidden layers}
\end{itemize}
\end{frame}

\begin{frame}{Neural networks - hidden layers}

We specialize hidden layers $F_{j}$ of a neural network
\[
    \phi(\xx; \VTheta) = A \circ F_{L} \circ \dots \circ F_1(\xx)
\]
to pairs "nonlinearity + affine map":
\[
    F_j(z_{j}) = \sigma(W_j \, z_{j-1} + b_j)
\]
where
\begin{itemize}
    \item \(W_j \in \R^{n_{j-1}} \times \R^{n_j}\) are \textbf{weights} matrices
    \item \(b_j \in \R^{n_j}\) are \textbf{bias} vectors
    \item nonlinear function \(\sigma\) is applied componentwise
\end{itemize}

Parameter vector $\VTheta$ is a concatenation of $A$, $W_{j}$ and $b_{j}$ for
$j = 1, \dots, L$.

\end{frame}

\begin{frame}
\frametitle{Visualization of multilayer perceptron $\R^2 \to \R$ with one hidden layer}

\begin{figure}[t]
    \centering
    \def\layersep{2.5cm}

    \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
        \tikzstyle{every pin edge}=[<-,shorten <=1pt]
        \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
        \tikzstyle{input neuron}=[neuron, fill=CGreenD];
        \tikzstyle{output neuron}=[neuron, fill=CRedD];
        \tikzstyle{hidden neuron}=[neuron, fill=CBlueD];
        \tikzstyle{annot} = [text width=4em, text centered]
    
        % Draw the input layer nodes
        \foreach \name / \y in {1,...,2}
        % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
            \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-1-\y) {};
    
        % Draw the hidden layer nodes
        \foreach \name / \y in {1,...,5}
            \path[yshift=0.5cm]
                node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};
    
        % Draw the output layer node
        \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};
    
        % Connect every node in the input layer with every node in the
        % hidden layer.
        \foreach \source in {1,...,2}
            \foreach \dest in {1,...,5}
                \path (I-\source) edge (H-\dest);
    
        % Connect every node in the hidden layer with the output layer
        \foreach \source in {1,...,5}
            \path (H-\source) edge (O);
    
        % Annotate the layers
        \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
        \node[annot,left of=hl] {Input layer};
        \node[annot,right of=hl] {Output layer};
    \end{tikzpicture}
\end{figure}

\begin{minipage}{\linewidth}
\centering
\tiny[\url{https://texample.net/tikz/examples/neural-network/}, with modifications]
\end{minipage}
\end{frame}
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
\begin{frame}{Training process and algorithms}

With the above choices for parameterized estimator $\phi(\xx; \VTheta_{\phi})$
and $\psi(\xx; \VTheta_{\psi})$, training process is
\[
\VTheta_{\phi}^{*}, \VTheta_{\psi}^{*} =
\arg \min_{\VTheta_\phi, \VTheta_\psi} \mathcal L(\phi, \vec \psi, \mathrm D)
  + \lambda R_{\perp}(\phi, \vec \psi) + \mu R_{\text S}(\phi, \vec \psi)
\]

\begin{itemize}
    \item We use ADAM optimizer with default parameters
    \item We set initial learning rate to 0.1 to help generalization
    \item We note that decrease learning rate after some number of iterations
        helps with stability and generalization
\end{itemize}
\end{frame}

% ------------------------------------------------------------------------------
\begin{frame}[standout]
\centering
\Huge
Thank you!
\end{frame}

\end{document}
